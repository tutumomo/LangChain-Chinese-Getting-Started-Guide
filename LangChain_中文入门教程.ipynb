{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQSLwy9FqC66"
      },
      "source": [
        "文章已开源，欢迎star\n",
        "https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLVXJRQ7tiBe"
      },
      "source": [
        "#装包以及初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH_o5pZM0pUj",
        "outputId": "838e65eb-a401-4e17-a2af-3b0eb9d01ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  d:\\TOMO.Project\\langchain\\venv\\Scripts\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  d:\\TOMO.Project\\langchain\\venv\\Scripts\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  d:\\TOMO.Project\\langchain\\venv\\Scripts\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
            "  d:\\TOMO.Project\\langchain\\venv\\Scripts\\python.exe -m pip install [options] [-e] <local project path> ...\n",
            "  d:\\TOMO.Project\\langchain\\venv\\Scripts\\python.exe -m pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai google-search-results unstructured chromadb pinecone-client youtube-transcript-api pytube\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kk01CyyA1GwX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxx'\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao78SDEp1Xbf"
      },
      "source": [
        "#使用 LangChain 完成一次问答"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "TSeOZmGJ1aL_",
        "outputId": "c1913fda-3416-4e93-b3e1-88eabb35684d"
      },
      "outputs": [],
      "source": [
        "\"\"\" 調用 openAI 的寫法 \n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\",max_tokens=1024)\n",
        "llm(\"怎麼評價人工智能\")\n",
        "\"\"\"\n",
        "\n",
        "# gemini 生成式 AI 模型\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
        "prompt_parts = [\"寫一個Python函數並向我解釋一下\"]\n",
        "response = model.generate_content(prompt_parts)\n",
        "print(response.text)\n",
        "# to_markdown(response.text) #這樣無法輸出??\n",
        "\n",
        "# gemini 對話模型\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = chat_model.invoke(\"保持健康的最佳做法是什麼？\")\n",
        "to_markdown(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0-Zs6KZ6Dgi"
      },
      "source": [
        "#通过 Google 搜索并返回答案"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W3VCMBux6Iuq"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "GfyZF-aW6RJa",
        "outputId": "cb696980-98f9-4518-975a-2024370c3caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find the date today and search for any significant events that occurred on this day in history.\n",
            "Action: Search\n",
            "Action Input: Today's date in Traditional Chinese\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSaturday, February 24, 2024\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow that I have today's date, I can search for significant events that took place on this day in history.\n",
            "Action: Search\n",
            "Action Input: Significant events that occurred on February 24 in history in Traditional Chinese\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['The Lantern Festival also called Shangyuan Festival is a Chinese traditional festival celebrated on the fifteenth day of the first month in the lunisolar ...', 'During the Tang Dynasty (618 – 907 AD), the Lantern Festival became an important cultural event that was celebrated by people from all walks of life as a time ...', 'Chinese Lantern Festival (Yuan Xiao Jie) falls on February 24, 2024. It is a festive day and the end of the Chinese New Year (the Spring ...', '484 – King Huneric of the Vandals replaces Nicene bishops with Arian ones, and banishes some to Corsica. · 1303 – The English are defeated at the Battle of ...', \"February 24th is World Bartender Day as well as Tortilla Chip Day. Welcome to the 55th day of the year! We're gradually getting through the year, ...\", 'The traditional Chinese festivals include Chinese New Year, the Lantern Festival, Tomb Sweeping Festival, Double Seven Festival and the ...', \"Portland's Asian communities celebrate the 2023 Lunar New Year with traditional dances, lantern ceremonies and stunning performances.\", 'February 24 significant news events for this day include Johnny Weissmuller Wins Olympic Golds, Voice of America Goes On the Air, Juan Peron Elected ...', \"Characterized by diverse styles and themes, traditional Chinese festivals are an important part of the country's history and culture, both ancient and modern.\", \"On This Day In History - February 24: anniversaries, birthdays, major events, and time capsules. This day's facts in the arts, politics, and sciences.\"]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 元宵節又稱上元節，是中國傳統節日之一，自古以來就以熱鬧喜慶的特色，被視為華人的狂歡節。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'元宵節又稱上元節，是中國傳統節日之一，自古以來就以熱鬧喜慶的特色，被視為華人的狂歡節。'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "# 加载 OpenAI 模型\n",
        "# llm = OpenAI(temperature=0,max_tokens=2048)\n",
        "\n",
        " # 加载 serpapi 工具\n",
        "tools = load_tools([\"serpapi\"])\n",
        "# 工具加载后都需要初始化，verbose 参数为 True，会打印全部的执行详情\n",
        "# 注意！！！使用的是 chat_model\n",
        "agent = initialize_agent(tools, chat_model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 运行 agent\n",
        "agent.run(\"What's the date today? What great events have taken place today in history? Please answer in Traditional Chinese.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1nWOkGvCStj"
      },
      "source": [
        "#对超长文本进行总结"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8WriJqBU6RDa",
        "outputId": "b45cd864-a2fb-4c36-b19d-f53c04a9b3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "documents:1\n",
            "documents:319\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mmax_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 创建总结链\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_summarize_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 执行总结链，（为了快速演示，只总结前5段）\u001b[39;00m\n\u001b[0;32m     31\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(split_documents[:\u001b[38;5;241m5\u001b[39m])\n",
            "File \u001b[1;32md:\\TOMO.Project\\langchain\\venv\\lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:160\u001b[0m, in \u001b[0;36mload_summarize_chain\u001b[1;34m(llm, chain_type, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](llm, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\TOMO.Project\\langchain\\venv\\lib\\site-packages\\langchain\\chains\\summarize\\__init__.py:119\u001b[0m, in \u001b[0;36m_load_refine_chain\u001b[1;34m(llm, question_prompt, refine_prompt, document_variable_name, initial_response_name, refine_llm, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_refine_chain\u001b[39m(\n\u001b[0;32m    110\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m    111\u001b[0m     question_prompt: BasePromptTemplate \u001b[38;5;241m=\u001b[39m refine_prompts\u001b[38;5;241m.\u001b[39mPROMPT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RefineDocumentsChain:\n\u001b[1;32m--> 119\u001b[0m     initial_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     _refine_llm \u001b[38;5;241m=\u001b[39m refine_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[0;32m    121\u001b[0m     refine_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m_refine_llm, prompt\u001b[38;5;241m=\u001b[39mrefine_prompt, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
            "File \u001b[1;32md:\\TOMO.Project\\langchain\\venv\\lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[1;32md:\\TOMO.Project\\langchain\\venv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "\n",
        "# 导入文本\n",
        "# loader = UnstructuredFileLoader(\"/content/sample_data/data/lg_test.txt\")\n",
        "loader = UnstructuredFileLoader(\"./data/lg_test.txt\")\n",
        "# 将文本转成 Document 对象\n",
        "document = loader.load()\n",
        "print(f'documents:{len(document)}')\n",
        "\n",
        "# 初始化文本分割器\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0\n",
        ")\n",
        "\n",
        "# 切分文本\n",
        "split_documents = text_splitter.split_documents(document)\n",
        "print(f'documents:{len(split_documents)}')\n",
        "\n",
        "# 加载 llm 模型,不用 openAI\n",
        "# llm = OpenAI(max_tokens=1500)\n",
        "# 改用 gemini\n",
        "model.max_tokens=1500\n",
        "# 创建总结链\n",
        "chain = load_summarize_chain(model, chain_type=\"refine\", verbose=True)\n",
        "\n",
        "# 执行总结链，（为了快速演示，只总结前5段）\n",
        "chain.run(split_documents[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySCEE0-tKw-"
      },
      "source": [
        "#构建本地知识库问答机器人"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGoTRXA5tUbW",
        "outputId": "b9523b8e-b170-4dd7-ef77-a04b07b0e66c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Directory not found: '/content/sample_data/data/'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/sample_data/data/\u001b[39m\u001b[38;5;124m'\u001b[39m, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**/*.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 将数据转成 document 对象，每个文件会作为一个 document\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 初始化加载器\u001b[39;00m\n\u001b[0;32m     14\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32md:\\TOMO.Project\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\directory.py:115\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected directory, got file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: Directory not found: '/content/sample_data/data/'"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI,VectorDBQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 加载文件夹中的所有txt类型的文件\n",
        "loader = DirectoryLoader('./data/', glob='**/*.txt')\n",
        "# 将数据转成 document 对象，每个文件会作为一个 document\n",
        "documents = loader.load()\n",
        "\n",
        "# 初始化加载器\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "# 切割加载的 document\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 初始化 openai 的 embeddings 对象\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# 将 document 通过 openai 的 embeddings 对象计算 embedding向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
        "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
        "\n",
        "# 创建问答对象\n",
        "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch,return_source_documents=True)\n",
        "# 进行问答\n",
        "result = qa({\"query\": \"科大讯飞今年第一季度收入是多少？\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gGjGMljB24P"
      },
      "source": [
        "# 构建向量索引数据库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "hSKnlX1AB-__",
        "outputId": "d4b5fa66-68fa-4fc1-dc92-81fdd764efa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "科大讯飞前三季营收曝光，用AI赋能企业转型推出了多款SaaS产品\n",
            "\n",
            "而在十月二十八日晚上，在一份关于科技的报告中，复旦讯飞公布了2022年第三季度的财报。据财报，今年第一季度，科技讯飞的收入为126.5亿人民币，较上年同期增加16.5%。该公司在应对新冠肺炎疫情的过程中，在其核心产业领域，持续取得了较好的发展。\n",
            "\n",
            "据该份财报，截至今年第三季度，科技讯飞的营运资金净流入率较去年增加百分之三十二，虽然受到了严峻的经济形势影响，但科技讯飞的营运资金和营业收入仍维持着强劲的发展势头。\n",
            "\n",
            "虽然收入增加了不少，但在这一点上，科大讯飞的高层也坦言，与过去几个财季相比，第一个财季的收入增加速度明显慢了下来，这其中最重要的一个因素，便是由于第一个财季受到了病毒的冲击，导致了国内不少工程进度推迟。在讯飞的智能教育商业计划中，第三季度有10个左右的延期，合约金额接近16个。另外，安徽省方面，为强化对省内规划工程的协调，在第三季度内，大部分大型工程的投标都已经停止，这也是造成今年第三季度，科大讯飞收入的增幅低于预计的主要因素。\n",
            "\n",
            "虽然收入增加了不少，但在这一点上，科大讯飞的高层也坦言，与过去几个财季相比，第一个财季的收入增加速度明显慢了下来，这其中最重要的一个因素，便是由于第一个财季受到了病毒的冲击，导致了国内不少工程进度推迟。在讯飞的智能教育商业计划中，第三季度有10个左右的延期，合约金额接近16个。另外，安徽省方面，为强化对省内规划工程的协调，在第三季度内，大部分大型工程的投标都已经停止，这也是造成今年第三季度，科大讯飞收入的增幅低于预计的主要因素。\n",
            "\n",
            "虽然收入增加了不少，但在这一点上，科大讯飞的高层也坦言，与过去几个财季相比，第一个财季的收入增加速度明显慢了下来，这其中最重要的一个因素，便是由于第一个财季受到了病毒的冲击，导致了国内不少工程进度推迟。在讯飞的智能教育商业计划中，第三季度有10个左右的延期，合约金额接近16个。另外，安徽省方面，为强化对省内规划工程的协调，在第三季度内，大部分大型工程的投标都已经停止，这也是造成今年第三季度，科大讯飞收入的增幅低于预计的主要因素。\n",
            "\n",
            "科大讯飞前三季营收曝光，用AI赋能企业转型推出了多款SaaS产品\n",
            "\n",
            "Question: 科大讯飞今年第一季度收入是多少？\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 根据财报，今年第一季度，科技讯飞的收入为126.5亿人民币。'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "import pinecone\n",
        "\n",
        "# 初始化 pinecone\n",
        "pinecone.init(\n",
        "  api_key=\"你的api key\",\n",
        "  environment=\"你的Environment\"\n",
        ")\n",
        "\n",
        "loader = DirectoryLoader('/content/sample_data/data/', glob='**/*.txt')\n",
        "# 将数据转成 document 对象，每个文件会作为一个 document\n",
        "documents = loader.load()\n",
        "\n",
        "# 初始化加载器\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "# 切割加载的 document\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 初始化 openai 的 embeddings 对象\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "index_name=\"liaokong-test\"\n",
        "# 持久化数据\n",
        "# docsearch = Pinecone.from_texts([t.page_content for t in split_docs], embeddings, index_name=index_name)\n",
        "\n",
        "# 加载数据\n",
        "docsearch = Pinecone.from_existing_index(index_name,embeddings)\n",
        "\n",
        "query = \"科大讯飞今年第一季度收入是多少？\"\n",
        "docs = docsearch.similarity_search(query, include_metadata=True)\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZGQ72NODIZ"
      },
      "source": [
        "#使用GPT3.5模型构建油管频道问答机器人\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "LRan7KZ-MgBv",
        "outputId": "3ea5d830-7ad4-4dc9-9251-f91a886a297b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "问题：这个视频讲了什么？\n",
            "这个视频是关于 Unreal Engine 5.2 的新实验功能的展示。在视频中介绍了一些新特性，如植被渲染、物理模拟、流体模拟、材质框架等。同时，还展示了一些环境场景的探索和汽车驾驶的游戏体验。视频还介绍了一些可用于生成可编程内容的实验工具。\n",
            "问题：对环境场景的探索有哪些？\n",
            "视频中展示了一个可编程的环境场景，包括树木、岩石、雾、虫子、鸟类等，同时还有一个四平方公里的区域可以探索，漫游和越野驾驶。视频还展示了汽车驾驶的游戏体验和实验工具的使用。\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c6bb568a9c8c>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'问题：'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0;31m# 开始发送问题 chat_history 为必须参数,用于存储对话历史\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chat_history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ChatVectorDBChain, ConversationalRetrievalChain\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "  ChatPromptTemplate,\n",
        "  SystemMessagePromptTemplate,\n",
        "  HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# 加载 youtube 频道\n",
        "loader = YoutubeLoader.from_youtube_channel('https://www.youtube.com/watch?v=Dj60HHy-Kqk')\n",
        "# 将数据转成 document\n",
        "documents = loader.load()\n",
        "\n",
        "# 初始化文本分割器\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=1000,\n",
        "  chunk_overlap=20\n",
        ")\n",
        "\n",
        "# 分割 youtube documents\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# 初始化 openai embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 将数据存入向量存储\n",
        "vector_store = Chroma.from_documents(documents, embeddings)\n",
        "# 通过向量存储初始化检索器\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "system_template = \"\"\"\n",
        "Use the following context to answer the user's question.\n",
        "If you don't know the answer, say you don't, don't try to make it up. And answer in Chinese.\n",
        "-----------\n",
        "{context}\n",
        "-----------\n",
        "{chat_history}\n",
        "\"\"\"\n",
        "\n",
        "# 构建初始 messages 列表，这里可以理解为是 openai 传入的 messages 参数\n",
        "messages = [\n",
        "  SystemMessagePromptTemplate.from_template(system_template),\n",
        "  HumanMessagePromptTemplate.from_template('{question}')\n",
        "]\n",
        "\n",
        "# 初始化 prompt 对象\n",
        "prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "# 初始化问答链\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1,max_tokens=2048),retriever,qa_prompt=prompt)\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "  question = input('问题：')\n",
        "  # 开始发送问题 chat_history 为必须参数,用于存储对话历史\n",
        "  result = qa({'question': question, 'chat_history': chat_history})\n",
        "  chat_history.append((question, result['answer']))\n",
        "  print(result['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRcPOsNT8mE3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVApiSmm8ocW"
      },
      "source": [
        "#用 OpenAI 连接万种工具"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMo9CZHr80nW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"ZAPIER_NLA_API_KEY\"] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJp8wx3Q8tDH",
        "outputId": "0bc1f87b-8074-4dd0-b11e-c51935ee18b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gmail: Find Email\n",
            "A wrapper around Zapier NLA actions. The input to this tool is a natural language instruction, for example \"get the latest email from my bank\" or \"send a slack message to the #general channel\". Each tool will have params associated with it that are specified as a list. You MUST take into account the params when creating the instruction. For example, if the params are ['Message_Text', 'Channel'], your instruction should be something like 'send a slack message to the #general channel with the text hello world'. Another example: if the params are ['Calendar', 'Search_Term'], your instruction should be something like 'find the meeting in my personal calendar at 3pm'. Do not make up params, they will be explicitly specified in the tool description. If you do not have enough information to fill in the params, just say 'not enough information provided in the instruction, missing <param>'. If you get a none or null response, STOP EXECUTION, do not try to another tool!This tool specifically used for: Gmail: Find Email, and has params: ['Search_String']\n",
            "\n",
            "\n",
            "\n",
            "Gmail: Send Email\n",
            "A wrapper around Zapier NLA actions. The input to this tool is a natural language instruction, for example \"get the latest email from my bank\" or \"send a slack message to the #general channel\". Each tool will have params associated with it that are specified as a list. You MUST take into account the params when creating the instruction. For example, if the params are ['Message_Text', 'Channel'], your instruction should be something like 'send a slack message to the #general channel with the text hello world'. Another example: if the params are ['Calendar', 'Search_Term'], your instruction should be something like 'find the meeting in my personal calendar at 3pm'. Do not make up params, they will be explicitly specified in the tool description. If you do not have enough information to fill in the params, just say 'not enough information provided in the instruction, missing <param>'. If you get a none or null response, STOP EXECUTION, do not try to another tool!This tool specifically used for: Gmail: Send Email, and has params: ['Cc', 'Body', 'To', 'Subject']\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents.agent_toolkits import ZapierToolkit\n",
        "from langchain.utilities.zapier import ZapierNLAWrapper\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=.3)\n",
        "\n",
        "zapier = ZapierNLAWrapper()\n",
        "toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\n",
        "agent = initialize_agent(toolkit.get_tools(), llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "# 我们可以通过打印的方式看到我们都在 Zapier 里面配置了哪些可以用的工具\n",
        "for tool in toolkit.get_tools():\n",
        "  print (tool.name)\n",
        "  print (tool.description)\n",
        "  print (\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-r5lPuV9j_1"
      },
      "outputs": [],
      "source": [
        "agent.run('请用中文总结最后一封\"*********@qq.com\"发给我的邮件。并将总结发送给\"*********@qq.com\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDdWamdTIw4-"
      },
      "source": [
        "# 一些有意思的小Tip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bUGvgAqJfkt"
      },
      "source": [
        "##执行多个chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQFxSP3rJlHF",
        "outputId": "6ba4cef5-28ff-4b4c-8a36-69ad8c19d8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mPasta alla Carbonara.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "Ingredients:\n",
            "- 2 cloves of garlic, minced \n",
            "- 2 tablespoons of olive oil \n",
            "- 2 cups of cooked spaghetti pasta \n",
            "- 4 slices of thick-cut bacon, cooked and chopped \n",
            "- 2 eggs, lightly beaten \n",
            "- 1/2 cup of Parmesan cheese, grated \n",
            "- Salt and black pepper to taste \n",
            "\n",
            "Directions:\n",
            "1. In a large skillet, heat the olive oil over medium heat and add the garlic, stirring until it is fragrant (about 2 minutes).\n",
            "2. Push the garlic to the side of the pan and add the cooked spaghetti pasta, stirring it around until all of the strands are coated with garlic and oil.\n",
            "3. Turn off the heat, add the cooked bacon and Parmesan cheese, and stir until combined.\n",
            "4. Add the beaten eggs and mix until evenly combined.\n",
            "5. Season to taste with salt and pepper, and serve warm. Enjoy!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# location 链\n",
        "llm = OpenAI(temperature=1)\n",
        "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# meal 链\n",
        "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# 通过 SimpleSequentialChain 串联起来，第一个答案会被替换第二个中的user_meal，然后再进行询问\n",
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
        "review = overall_chain.run(\"Rome\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QVu6_HVJTlW"
      },
      "source": [
        "##结构化输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwEocODtt5Tm",
        "outputId": "c7c36e6b-0532-47a0-fc5d-5c729eab2c55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "# 告诉他我们生成的内容需要哪些字段，每个字段类型式啥\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
        "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
        "]\n",
        "\n",
        "# 初始化解析器\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# 生成的格式提示符\n",
        "# {\n",
        "#\t\"bad_string\": string  // This a poorly formatted user input string\n",
        "#\t\"good_string\": string  // This is your response, a reformatted response\n",
        "#}\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "template = \"\"\"\n",
        "You will be given a poorly formatted string from a user.\n",
        "Reformat it and make sure all the words are spelled correctly\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "# 讲我们的格式描述嵌入到 prompt 中去，告诉 llm 我们需要他输出什么样格式的内容\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template\n",
        ")\n",
        "\n",
        "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
        "llm_output = llm(promptValue)\n",
        "\n",
        "# 使用解析器进行解析生成的内容\n",
        "output_parser.parse(llm_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00kLaQZWS-Jq"
      },
      "source": [
        "##爬取网页并输出JSON数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAkS_wX8IaA1",
        "outputId": "75653da4-6b2b-4d9a-c2b7-5ec5a7619ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"company_name\":\"贵州茅台酒股份有限公司\",\n",
            "  \"company_english_name\":\"Kweichow Moutai Co.,Ltd.\",\n",
            "  \"issue_price\":\"31.39\",\n",
            "  \"date_of_establishment\":\"1999-11-20\",\n",
            "  \"registered_capital\":\"125620万元(CNY)\",\n",
            "  \"office_address\":\"贵州省仁怀市茅台镇\",\n",
            "  \"Company_profile\":\"公司是根据贵州省人民政府黔府函〔1999〕291号文,由中国贵州茅台酒厂有限责任公司作为主发起人,联合贵州茅台酒厂技术开发公司、贵州省轻纺集体工业联社、深圳清华大学研究院、中国食品发酵工业研究院、北京市糖业烟酒公司、江苏省糖烟酒总公司、上海捷强烟草糖酒(集团)有限公司于1999年11月20日共同发起设立的股份有限公司。经中国证监会证监发行字[2001]41号文核准并按照财政部企[2001]56号文件的批复,公司于2001年7月31日在上海证券交易所公开发行7,150万(其中,国有股存量发行650万股)A股股票。主营业务：贵州茅台酒系列产品的生产与销售,饮料、食品、包装材料的生产与销售,防伪技术开发;信息产业相关产品的研制和开发等。\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMRequestsChain, LLMChain\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template = \"\"\"在 >>> 和 <<< 之间是网页的返回的HTML内容。\n",
        "网页是新浪财经A股上市公司的公司简介。\n",
        "请抽取参数请求的信息。\n",
        "\n",
        ">>> {requests_result} <<<\n",
        "请使用如下的JSON格式返回数据\n",
        "{{\n",
        "  \"company_name\":\"a\",\n",
        "  \"company_english_name\":\"b\",\n",
        "  \"issue_price\":\"c\",\n",
        "  \"date_of_establishment\":\"d\",\n",
        "  \"registered_capital\":\"e\",\n",
        "  \"office_address\":\"f\",\n",
        "  \"Company_profile\":\"g\"\n",
        "\n",
        "}}\n",
        "Extracted:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"requests_result\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chain = LLMRequestsChain(llm_chain=LLMChain(llm=llm, prompt=prompt))\n",
        "inputs = {\n",
        "  \"url\": \"https://vip.stock.finance.sina.com.cn/corp/go.php/vCI_CorpInfo/stockid/600519.phtml\"\n",
        "}\n",
        "\n",
        "response = chain(inputs)\n",
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dh9Zxt6NqAY"
      },
      "source": [
        "##自定义工具"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "IHMHz8vfNs6w",
        "outputId": "b6ce681a-3eed-4e69-95ad-6fa76ee7429e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
            "Action: Search\n",
            "Action Input: \"Leo DiCaprio girlfriend\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLeonardo DiCaprio has split from girlfriend Camila Morrone. Getty. The Titanic actor hasn't been in a relationship with a woman over the age of ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out Camila Morrone's age\n",
            "Action: Search\n",
            "Action Input: \"Camila Morrone age\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m25 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the age and can calculate the power\n",
            "Action: Calculator\n",
            "Action Input: 25^0.43\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "25^0.43\u001b[32;1m\u001b[1;3m\n",
            "```python\n",
            "import math\n",
            "print(math.pow(25, 0.43))\n",
            "```\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m3.991298452658078\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.991298452658078\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Camila Morrone's age raised to the 0.43 power is 3.991298452658078.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Camila Morrone's age raised to the 0.43 power is 3.991298452658078.\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMMathChain, SerpAPIWrapper\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# 初始化搜索链和计算链\n",
        "search = SerpAPIWrapper()\n",
        "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
        "\n",
        "# 生成一个功能列表，指明这个 agent 里面都有哪些可用工具\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math_chain.run,\n",
        "        description=\"useful for when you need to answer questions about math\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# 初始化 agent\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 执行 agent\n",
        "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOIICEUZvjR0"
      },
      "source": [
        "##使用Memory实现一个带记忆的对话机器人"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qG5EGbpviKl",
        "outputId": "48541499-78c9-40ca-ee94-6e6a4c9e414d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='中国的首都是北京。' additional_kwargs={}\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# 初始化 MessageHistory 对象\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "# 给 MessageHistory 对象添加对话内容\n",
        "history.add_ai_message(\"你好！\")\n",
        "history.add_user_message(\"中国的首都是哪里？\")\n",
        "\n",
        "# 执行对话\n",
        "ai_response = chat(history.messages)\n",
        "print(ai_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mLVXJRQ7tiBe",
        "Ao78SDEp1Xbf",
        "j0-Zs6KZ6Dgi",
        "L1nWOkGvCStj",
        "NySCEE0-tKw-",
        "7gGjGMljB24P",
        "dhZGQ72NODIZ",
        "CVApiSmm8ocW",
        "3bUGvgAqJfkt",
        "5QVu6_HVJTlW",
        "00kLaQZWS-Jq",
        "2dh9Zxt6NqAY",
        "dOIICEUZvjR0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
